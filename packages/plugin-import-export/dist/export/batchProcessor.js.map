{"version":3,"sources":["../../src/export/batchProcessor.ts"],"sourcesContent":["/**\n * Export-specific batch processor for processing documents in batches during export.\n * Uses the generic batch processing utilities from useBatchProcessor.\n */\nimport type { PayloadRequest, SelectType, Sort, TypedUser, Where } from 'payload'\n\nimport { type BatchProcessorOptions } from '../utilities/useBatchProcessor.js'\n\n/**\n * Export-specific batch processor options\n */\nexport interface ExportBatchProcessorOptions extends BatchProcessorOptions {\n  debug?: boolean\n}\n\n/**\n * Find arguments for querying documents during export\n */\nexport interface ExportFindArgs {\n  collection: string\n  depth: number\n  draft: boolean\n  limit: number\n  locale?: string\n  overrideAccess: boolean\n  page?: number\n  select?: SelectType\n  sort?: Sort\n  user?: TypedUser\n  where?: Where\n}\n\n/**\n * Options for processing an export operation\n */\nexport interface ExportProcessOptions<TDoc = unknown> {\n  /**\n   * The slug of the collection to export\n   */\n  collectionSlug: string\n  /**\n   * Arguments to pass to payload.find()\n   */\n  findArgs: ExportFindArgs\n  /**\n   * The export format - affects column tracking for CSV\n   */\n  format: 'csv' | 'json'\n  /**\n   * Maximum number of documents to export\n   */\n  maxDocs: number\n  /**\n   * The Payload request object\n   */\n  req: PayloadRequest\n  /**\n   * Starting page for pagination (default: 1)\n   */\n  startPage?: number\n  /**\n   * Transform function to apply to each document\n   */\n  transformDoc: (doc: TDoc) => Record<string, unknown>\n}\n\n/**\n * Result from processing an export operation\n */\nexport interface ExportResult {\n  /**\n   * Discovered column names (for CSV exports)\n   */\n  columns: string[]\n  /**\n   * Transformed documents ready for output\n   */\n  docs: Record<string, unknown>[]\n  /**\n   * Total number of documents fetched\n   */\n  fetchedCount: number\n}\n\n/**\n * Creates an export batch processor with the specified options.\n *\n * @param options - Configuration options for the batch processor\n * @returns An object containing the processExport method\n *\n * @example\n * ```ts\n * const processor = createExportBatchProcessor({ batchSize: 100, debug: true })\n *\n * const result = await processor.processExport({\n *   collectionSlug: 'posts',\n *   findArgs: { collection: 'posts', depth: 1, draft: false, limit: 100, overrideAccess: false },\n *   format: 'csv',\n *   maxDocs: 1000,\n *   req,\n *   transformDoc: (doc) => flattenObject({ doc }),\n * })\n * ```\n */\nexport function createExportBatchProcessor(options: ExportBatchProcessorOptions = {}) {\n  const batchSize = options.batchSize ?? 100\n  const debug = options.debug ?? false\n\n  /**\n   * Process an export operation by fetching and transforming documents in batches.\n   *\n   * @param processOptions - Options for the export operation\n   * @returns The export result containing transformed documents and column info\n   */\n  const processExport = async <TDoc>(\n    processOptions: ExportProcessOptions<TDoc>,\n  ): Promise<ExportResult> => {\n    const { findArgs, format, maxDocs, req, startPage = 1, transformDoc } = processOptions\n\n    const docs: Record<string, unknown>[] = []\n    const columnsSet = new Set<string>()\n    const columns: string[] = []\n\n    let currentPage = startPage\n    let fetched = 0\n    let hasNextPage = true\n\n    while (hasNextPage) {\n      const remaining = Math.max(0, maxDocs - fetched)\n\n      if (remaining === 0) {\n        break\n      }\n\n      const result = await req.payload.find({\n        ...findArgs,\n        limit: Math.min(batchSize, remaining),\n        page: currentPage,\n      })\n\n      if (debug) {\n        req.payload.logger.debug(\n          `Processing export batch ${currentPage} with ${result.docs.length} documents`,\n        )\n      }\n\n      for (const doc of result.docs) {\n        const transformedDoc = transformDoc(doc as TDoc)\n        docs.push(transformedDoc)\n\n        // Track columns for CSV format\n        if (format === 'csv') {\n          for (const key of Object.keys(transformedDoc)) {\n            if (!columnsSet.has(key)) {\n              columnsSet.add(key)\n              columns.push(key)\n            }\n          }\n        }\n      }\n\n      fetched += result.docs.length\n      hasNextPage = result.hasNextPage && fetched < maxDocs\n      currentPage++\n    }\n\n    return { columns, docs, fetchedCount: fetched }\n  }\n\n  /**\n   * Async generator for streaming export - yields batches instead of collecting all.\n   * Useful for streaming exports where you want to process batches as they're fetched.\n   *\n   * @param processOptions - Options for the export operation\n   * @yields Batch results containing transformed documents and discovered columns\n   *\n   * @example\n   * ```ts\n   * const processor = createExportBatchProcessor({ batchSize: 100 })\n   *\n   * for await (const batch of processor.streamExport({ ... })) {\n   *   // Process each batch as it's yielded\n   *   console.log(`Got ${batch.docs.length} documents`)\n   * }\n   * ```\n   */\n  async function* streamExport<TDoc>(\n    processOptions: ExportProcessOptions<TDoc>,\n  ): AsyncGenerator<{ columns: string[]; docs: Record<string, unknown>[] }> {\n    const { findArgs, format, maxDocs, req, startPage = 1, transformDoc } = processOptions\n\n    const columnsSet = new Set<string>()\n    const columns: string[] = []\n\n    let currentPage = startPage\n    let fetched = 0\n    let hasNextPage = true\n\n    while (hasNextPage) {\n      const remaining = Math.max(0, maxDocs - fetched)\n\n      if (remaining === 0) {\n        break\n      }\n\n      const result = await req.payload.find({\n        ...findArgs,\n        limit: Math.min(batchSize, remaining),\n        page: currentPage,\n      })\n\n      if (debug) {\n        req.payload.logger.debug(\n          `Streaming export batch ${currentPage} with ${result.docs.length} documents`,\n        )\n      }\n\n      const batchDocs: Record<string, unknown>[] = []\n\n      for (const doc of result.docs) {\n        const transformedDoc = transformDoc(doc as TDoc)\n        batchDocs.push(transformedDoc)\n\n        // Track columns for CSV format\n        if (format === 'csv') {\n          for (const key of Object.keys(transformedDoc)) {\n            if (!columnsSet.has(key)) {\n              columnsSet.add(key)\n              columns.push(key)\n            }\n          }\n        }\n      }\n\n      yield { columns: [...columns], docs: batchDocs }\n\n      fetched += result.docs.length\n      hasNextPage = result.hasNextPage && fetched < maxDocs\n      currentPage++\n    }\n  }\n\n  /**\n   * Discover all columns from the dataset by scanning through all batches.\n   * Useful for CSV exports where you need to know all columns before streaming.\n   *\n   * @param processOptions - Options for the export operation\n   * @returns Array of discovered column names\n   */\n  const discoverColumns = async <TDoc>(\n    processOptions: ExportProcessOptions<TDoc>,\n  ): Promise<string[]> => {\n    const { findArgs, maxDocs, req, startPage = 1, transformDoc } = processOptions\n\n    const columnsSet = new Set<string>()\n    const columns: string[] = []\n\n    let currentPage = startPage\n    let fetched = 0\n    let hasNextPage = true\n\n    while (hasNextPage) {\n      const remaining = Math.max(0, maxDocs - fetched)\n\n      if (remaining === 0) {\n        break\n      }\n\n      const result = await req.payload.find({\n        ...findArgs,\n        limit: Math.min(batchSize, remaining),\n        page: currentPage,\n      })\n\n      if (debug) {\n        req.payload.logger.debug(\n          `Scanning columns from batch ${currentPage} with ${result.docs.length} documents`,\n        )\n      }\n\n      for (const doc of result.docs) {\n        const transformedDoc = transformDoc(doc as TDoc)\n\n        for (const key of Object.keys(transformedDoc)) {\n          if (!columnsSet.has(key)) {\n            columnsSet.add(key)\n            columns.push(key)\n          }\n        }\n      }\n\n      fetched += result.docs.length\n      hasNextPage = result.hasNextPage && fetched < maxDocs\n      currentPage++\n    }\n\n    if (debug) {\n      req.payload.logger.debug(`Discovered ${columns.length} columns`)\n    }\n\n    return columns\n  }\n\n  return {\n    discoverColumns,\n    processExport,\n    streamExport,\n  }\n}\n"],"names":["createExportBatchProcessor","options","batchSize","debug","processExport","processOptions","findArgs","format","maxDocs","req","startPage","transformDoc","docs","columnsSet","Set","columns","currentPage","fetched","hasNextPage","remaining","Math","max","result","payload","find","limit","min","page","logger","length","doc","transformedDoc","push","key","Object","keys","has","add","fetchedCount","streamExport","batchDocs","discoverColumns"],"mappings":"AAAA;;;CAGC,GAiFD;;;;;;;;;;;;;;;;;;;CAmBC,GACD,OAAO,SAASA,2BAA2BC,UAAuC,CAAC,CAAC;IAClF,MAAMC,YAAYD,QAAQC,SAAS,IAAI;IACvC,MAAMC,QAAQF,QAAQE,KAAK,IAAI;IAE/B;;;;;GAKC,GACD,MAAMC,gBAAgB,OACpBC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,MAAM,EAAEC,OAAO,EAAEC,GAAG,EAAEC,YAAY,CAAC,EAAEC,YAAY,EAAE,GAAGN;QAExE,MAAMO,OAAkC,EAAE;QAC1C,MAAMC,aAAa,IAAIC;QACvB,MAAMC,UAAoB,EAAE;QAE5B,IAAIC,cAAcN;QAClB,IAAIO,UAAU;QACd,IAAIC,cAAc;QAElB,MAAOA,YAAa;YAClB,MAAMC,YAAYC,KAAKC,GAAG,CAAC,GAAGb,UAAUS;YAExC,IAAIE,cAAc,GAAG;gBACnB;YACF;YAEA,MAAMG,SAAS,MAAMb,IAAIc,OAAO,CAACC,IAAI,CAAC;gBACpC,GAAGlB,QAAQ;gBACXmB,OAAOL,KAAKM,GAAG,CAACxB,WAAWiB;gBAC3BQ,MAAMX;YACR;YAEA,IAAIb,OAAO;gBACTM,IAAIc,OAAO,CAACK,MAAM,CAACzB,KAAK,CACtB,CAAC,wBAAwB,EAAEa,YAAY,MAAM,EAAEM,OAAOV,IAAI,CAACiB,MAAM,CAAC,UAAU,CAAC;YAEjF;YAEA,KAAK,MAAMC,OAAOR,OAAOV,IAAI,CAAE;gBAC7B,MAAMmB,iBAAiBpB,aAAamB;gBACpClB,KAAKoB,IAAI,CAACD;gBAEV,+BAA+B;gBAC/B,IAAIxB,WAAW,OAAO;oBACpB,KAAK,MAAM0B,OAAOC,OAAOC,IAAI,CAACJ,gBAAiB;wBAC7C,IAAI,CAAClB,WAAWuB,GAAG,CAACH,MAAM;4BACxBpB,WAAWwB,GAAG,CAACJ;4BACflB,QAAQiB,IAAI,CAACC;wBACf;oBACF;gBACF;YACF;YAEAhB,WAAWK,OAAOV,IAAI,CAACiB,MAAM;YAC7BX,cAAcI,OAAOJ,WAAW,IAAID,UAAUT;YAC9CQ;QACF;QAEA,OAAO;YAAED;YAASH;YAAM0B,cAAcrB;QAAQ;IAChD;IAEA;;;;;;;;;;;;;;;;GAgBC,GACD,gBAAgBsB,aACdlC,cAA0C;QAE1C,MAAM,EAAEC,QAAQ,EAAEC,MAAM,EAAEC,OAAO,EAAEC,GAAG,EAAEC,YAAY,CAAC,EAAEC,YAAY,EAAE,GAAGN;QAExE,MAAMQ,aAAa,IAAIC;QACvB,MAAMC,UAAoB,EAAE;QAE5B,IAAIC,cAAcN;QAClB,IAAIO,UAAU;QACd,IAAIC,cAAc;QAElB,MAAOA,YAAa;YAClB,MAAMC,YAAYC,KAAKC,GAAG,CAAC,GAAGb,UAAUS;YAExC,IAAIE,cAAc,GAAG;gBACnB;YACF;YAEA,MAAMG,SAAS,MAAMb,IAAIc,OAAO,CAACC,IAAI,CAAC;gBACpC,GAAGlB,QAAQ;gBACXmB,OAAOL,KAAKM,GAAG,CAACxB,WAAWiB;gBAC3BQ,MAAMX;YACR;YAEA,IAAIb,OAAO;gBACTM,IAAIc,OAAO,CAACK,MAAM,CAACzB,KAAK,CACtB,CAAC,uBAAuB,EAAEa,YAAY,MAAM,EAAEM,OAAOV,IAAI,CAACiB,MAAM,CAAC,UAAU,CAAC;YAEhF;YAEA,MAAMW,YAAuC,EAAE;YAE/C,KAAK,MAAMV,OAAOR,OAAOV,IAAI,CAAE;gBAC7B,MAAMmB,iBAAiBpB,aAAamB;gBACpCU,UAAUR,IAAI,CAACD;gBAEf,+BAA+B;gBAC/B,IAAIxB,WAAW,OAAO;oBACpB,KAAK,MAAM0B,OAAOC,OAAOC,IAAI,CAACJ,gBAAiB;wBAC7C,IAAI,CAAClB,WAAWuB,GAAG,CAACH,MAAM;4BACxBpB,WAAWwB,GAAG,CAACJ;4BACflB,QAAQiB,IAAI,CAACC;wBACf;oBACF;gBACF;YACF;YAEA,MAAM;gBAAElB,SAAS;uBAAIA;iBAAQ;gBAAEH,MAAM4B;YAAU;YAE/CvB,WAAWK,OAAOV,IAAI,CAACiB,MAAM;YAC7BX,cAAcI,OAAOJ,WAAW,IAAID,UAAUT;YAC9CQ;QACF;IACF;IAEA;;;;;;GAMC,GACD,MAAMyB,kBAAkB,OACtBpC;QAEA,MAAM,EAAEC,QAAQ,EAAEE,OAAO,EAAEC,GAAG,EAAEC,YAAY,CAAC,EAAEC,YAAY,EAAE,GAAGN;QAEhE,MAAMQ,aAAa,IAAIC;QACvB,MAAMC,UAAoB,EAAE;QAE5B,IAAIC,cAAcN;QAClB,IAAIO,UAAU;QACd,IAAIC,cAAc;QAElB,MAAOA,YAAa;YAClB,MAAMC,YAAYC,KAAKC,GAAG,CAAC,GAAGb,UAAUS;YAExC,IAAIE,cAAc,GAAG;gBACnB;YACF;YAEA,MAAMG,SAAS,MAAMb,IAAIc,OAAO,CAACC,IAAI,CAAC;gBACpC,GAAGlB,QAAQ;gBACXmB,OAAOL,KAAKM,GAAG,CAACxB,WAAWiB;gBAC3BQ,MAAMX;YACR;YAEA,IAAIb,OAAO;gBACTM,IAAIc,OAAO,CAACK,MAAM,CAACzB,KAAK,CACtB,CAAC,4BAA4B,EAAEa,YAAY,MAAM,EAAEM,OAAOV,IAAI,CAACiB,MAAM,CAAC,UAAU,CAAC;YAErF;YAEA,KAAK,MAAMC,OAAOR,OAAOV,IAAI,CAAE;gBAC7B,MAAMmB,iBAAiBpB,aAAamB;gBAEpC,KAAK,MAAMG,OAAOC,OAAOC,IAAI,CAACJ,gBAAiB;oBAC7C,IAAI,CAAClB,WAAWuB,GAAG,CAACH,MAAM;wBACxBpB,WAAWwB,GAAG,CAACJ;wBACflB,QAAQiB,IAAI,CAACC;oBACf;gBACF;YACF;YAEAhB,WAAWK,OAAOV,IAAI,CAACiB,MAAM;YAC7BX,cAAcI,OAAOJ,WAAW,IAAID,UAAUT;YAC9CQ;QACF;QAEA,IAAIb,OAAO;YACTM,IAAIc,OAAO,CAACK,MAAM,CAACzB,KAAK,CAAC,CAAC,WAAW,EAAEY,QAAQc,MAAM,CAAC,QAAQ,CAAC;QACjE;QAEA,OAAOd;IACT;IAEA,OAAO;QACL0B;QACArC;QACAmC;IACF;AACF"}